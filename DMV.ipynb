{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# DMV Analysis and predictions"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "import pandas as pd #general manipulation\n",
                "from datetime import datetime, timedelta\n",
                "import pickle #loads model\n",
                "import ipywidgets as widgets #intercative model selection\n",
                "import constants #local file containing information about the dataset"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Intro\n",
                "#### Motivation\n",
                "The DMV is an entity that provides essential services such as providing/renewing driver’s licenses and IDs.\n",
                "\n",
                "During the 2020 pandemic the NJ DMV stopped accepting walk-ins and all visits needed to be booked online ahead of time. Since the system transitioned to be online only, frequently there are no available appointments for the following 90 days and one needs to keep trying to schedule an appointment.\n",
                "The model developed for this project aims to help people to schedule their appointments by providing them a better window of time to go online and book the service they need. \n",
                "\n",
                "This is a personal project, one that started when I tried to renew my driver's license and couldn't because there wasn't available times.\n",
                "\n",
                "The NJ MVC Appointment Scheduling can be found <a href=\"https://telegov.njportal.com/njmvc/AppointmentWizard\">here</a>."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Project structure\n",
                "This project is split in three main parts: \n",
                "* data collection;\n",
                "* data manipulation and model training, and\n",
                "* project description and model deployment. \n",
                "\n",
                "The reason for this division is that having the data collection as its own file allows it to be run in parallel and that havig the model deployment segregated allows is to run without the dataset present."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Data collection\n",
                "The project starts by scrapping the DMV website using Python and Selenium to get the next appointment available for all locations for all services at a given time and storing them in a SQL database. This data collection was being executed every approximately two minutes, a balance between not stressing the website, having too many duplicate entries and data granularity. This scrip run on Raspberry Pi in parallel to any analysis and was executed for a few months."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Manipulation steps"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Data structure\n",
                "Each service has a list of all locations, even if a given location didn't offer that specific service during the data collection time window. \n",
                "Each location for each service had its very next available appointment collected every approximately two minutes, and if no available appointment existed, it was completed with a NULL.\n",
                "\n",
                "### Extracting data structure\n",
                "The sqlite_schema was used to retrieve all services, as each one is a table in the database. From the tables, all columns names were retrieved to get all location names.\n",
                "\n",
                "### Cleaning NULLS\n",
                "A SQL query was executed to replace the words NULLs with the actual NULL values.\n",
                "\n",
                "### Retrieving the right time\n",
                "The shift (am/pm) information was lost during the scrapping data, hence they needed to be reintroduced to the SQL. Some rules were applied to find the missing information, like repeated hours for the same day are always in the afternoon and the first hour after a new day are in the morning.\n",
                "\n",
                "### Removing locations that don't have any appointment\n",
                "Not all locations provide all services, and the ones without any entry were removed from that service. They could also be removed because during the time the data was collected no new appointment was created. No model can be trained without data, so their removal is necessary in either case.\n",
                "\n",
                "### Dealing with missing data\n",
                "The data contains certain periods of time missing, either because the website or the scrapping script were down. The script will will find the next available appointment for each entry and calculate how much time it takes until it; if an appointment is available, this distance is zero.\n",
                "Just before a gap in measurement, the distance will either be zero (appointment available) or it will calculate the distance in hours until the next appointment. This can introduce an outlier depending of the time elapsed without data collection and to avoid it a fake appointment was created as last value before a gap. This also introduces an error, but it is less variable than an unknown gap.\n",
                "\n",
                "Finally, a fake event is created as the very last entry of each location beacuse the data will later be \"stacked\" and all columns will be merged. This step is to prevent leaking information between locations, because the next available appointment would be located in the next location.\n",
                "    \n",
                "### Stacking by melting\n",
                "it was decided to stack all columns together and have one column with all the next appointments and one with the location labels instead of one column per \n",
                "location to make it easier to apply functions to calculate functions of interest, like distance between next appointments and counts.\n",
                "\n",
                "### Calculating the next appointment\n",
                "A countdown of lines was calculated for each value that returns when the next appointmnet is going to take place. If it exists for that time, the difference is zero. This countdown is used to get the next appointmnet and finally the time it will take for the appointment to become available. ***This will be the value to be predicted by the model***. At this point all columns that will not be used were dropped.\n",
                "\n",
                "### Checking for FFT (tentative)\n",
                "Fast Fourier Transform was applied for checking if any frequency of appointments occured in the data and if it could be used for predicting the next appointments. The results were inconclusive and the method was aborted.\n",
                "\n",
                "### Adding categorical values for dates\n",
                "Finally, the weekdays, hours and minutes were transformed into categorical columns for appling the final model."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Model construction"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Parameters\n",
                "Finally, the model was built by using the categorical time columns and predicting when the next appointment would be available. \n",
                "\n",
                "Days, hours and minutes were one hot encoded to serve as input. Two main kinds of categorical transformation for days were possible: day of the month and weekday. Weekdays returned better results, which indicates knowing how far in the week one is when trying to schedule an appointment is more usefull than having the day of the month information available.\n",
                "\n",
                "### Construction\n",
                "A custom class and factory were built for the model:\n",
                "- The factory contained the instructions for one hot encoding and a blank model with pre-determined hyperparameters in a form of a pipeline.\n",
                "- The class received the cleaned data from the earlier steps, split it by location and called the factory. As it dynamically called new blank models, each one was independent from each other, which was used to predict different services and locations.\n",
                "\n",
                "The idea is to apply the model \n",
                "\n",
                "### Hyperparameters\n",
                "Once the model was ready, a GridSearchCV was run on several sets of hyperparameters and the one with best score was chosen. In order to use the GridSearchCV the dataset was split by location and the one hot encoder transformation was manually applied.\n",
                "\n",
                "### Evaluation\n",
                "As a timeseries has time as dependent variable, the dataset was split into the first 80% and the last 20% of observations, to avoid data leakage from the future to present.\n",
                "\n",
                "The scope of this project is to make one model for all services and locations, hence the evaluation was made on one location at a time.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Model deployment"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Final choice\n",
                "After optimizing the model with random forest regressor and ridge, the Root Mean Square Error (RMSE) was calculated for each one. The ridge model had an error as small as the random forest, with the advantage of being faster and easier to explain and faster"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "w = widgets.Dropdown(\n",
                "    options=constants.locations, #locations stored in a file called constants\n",
                "    value='Bakers_Basin',\n",
                "    description='Number:',\n",
                "    disabled=False,\n",
                ")\n",
                "display(w)"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Dropdown(description='Number:', index=1, options=('time_stamp', 'Bakers_Basin', 'Bayonne', 'Camden', 'Cherry_H…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "f96a5908bbc344db81cd6572fc286ed8"
                        }
                    },
                    "metadata": {}
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Loads models"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "w.value"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'Camden'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 52
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "source": [
                "model_name = 'initial_permit_' + w.value + '_model' # \n",
                "try:\n",
                "    model = pickle.load(open(model_name, 'rb'))\n",
                "    now = datetime.today() #prediction will be based on current time\n",
                "    df_predict = pd.DataFrame({'days_cat': [now.weekday()], 'hours_cat':[now.hour], 'minutes_cat':[now.minute]}) #now.weekday()\n",
                "    [results] = model.predict(df_predict) / 3.6e+12 #nano seconds to hours\n",
                "    predicted_time = now + timedelta(hours = results)\n",
                "    print('The best time to try to schedule an appointment is: '\\\n",
                "        + predicted_time.strftime('%x, %I:%m %p'))\n",
                "except FileNotFoundError:\n",
                "    model = 'no_model'\n",
                "    print('no model exists for that combination. Does that location provides that service?')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "The best time to try to schedule an appointment is: 05/30/22, 02:05 PM\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Conclusion"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "This model was able to predict a better time for returning to the website and scheduling an appointment online for a given location. The services offered by them are often vital and this tool aims to help people by providing them a better chance to use these services.\n",
                "\n",
                "The dataset was collected during the period of five months and there is room for improvement; the data is still being collected and can be used to improve the model.\n",
                "\n",
                "The final model used was a linear one, as it provided an error as low as a more complex models. It has the advantage of having a higher explainability and a smaller training time."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "*Cesar Krischer, 2022*"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.5 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "5d95667adeea69ab860892b2fc8ecb2b384e71d148151ab8326cdf0bf7a14c73"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}